{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dataset for instruction tuning\n",
    "\n",
    "This notebook will guide you through the process of generating a dataset for instruction tuning. We'll use the `distilabel` package to generate a dataset for instruction tuning.\n",
    "\n",
    "So let's dig in to some instruction tuning datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Generate a dataset for instruction tuning</h2>\n",
    "    <p>Now that you've seen how to generate a dataset for instruction tuning, try generating a dataset for instruction tuning.</p>\n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>ğŸ¢ Generate an instruction tuning dataset</p>\n",
    "    <p>ğŸ• Generate a dataset for instruction tuning with seed data</p>\n",
    "    <p>ğŸ¦ Generate a dataset for instruction tuning with seed data and with instruction evolution</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Install dependencies\n",
    "\n",
    "Instead of transformers, you can also install `vllm` or `hf-inference-endpoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[hf-transformers,outlines,instructor]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start synthesizing\n",
    "\n",
    "As we've seen in the previous course content, we can create a distilabel pipelines for instruction dataset generation. The bare minimum pipline is already provided. Make sure to scale up this pipeline to generate a large dataset for instruction tuning. Swap out models, model providers and generation arguments to see how they affect the quality of the dataset. Experiment small, scale up later.\n",
    "\n",
    "Check out the [distilabel components gallery](https://distilabel.argilla.io/latest/components-gallery/) for information about the processing classes and how to use them. \n",
    "\n",
    "An example of loading data from the Hub instead of dictionaries is provided below.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "with Pipeline(...) as pipeline:\n",
    "    ...\n",
    "\n",
    "if __name__ == \"__main__:\n",
    "    dataset = load_dataset(\"my-dataset\", split=\"train\")\n",
    "    distiset = pipeline.run(dataset=dataset)\n",
    "```\n",
    "\n",
    "Don't forget to push your dataset to the Hub after running the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.llms import TransformersLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import LoadDataFromDicts\n",
    "from distilabel.steps.tasks import TextGeneration\n",
    "\n",
    "with Pipeline() as pipeline:\n",
    "    data = LoadDataFromDicts(data=[{\"instruction\": \"Generate a short question about the Hugging Face Smol-Course.\"}])\n",
    "    llm = TransformersLLM(model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "    gen_a = TextGeneration(llm=llm, output_mappings={\"generation\": \"instruction\"})\n",
    "    gen_b = TextGeneration(llm=llm, output_mappings={\"generation\": \"response\"})\n",
    "    data >> gen_a >> gen_b\n",
    "\n",
    "distiset = pipeline.run(use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the main focus of the Hugging Face Smol-Course?\n",
      "The main focus of the Hugging Face Smol-Course is to provide an introduction to deep learning and natural language processing (NLP) using Python. It covers topics such as data preprocessing, model training, and evaluation, with a special emphasis on practical applications in NLP tasks like text classification, sentiment analysis, and machine translation. The course also includes hands-on projects that allow learners to apply their knowledge and gain practical experience.\n"
     ]
    }
   ],
   "source": [
    "print(distiset[\"default\"][\"train\"][0][\"instruction\"])\n",
    "print(distiset[\"default\"][\"train\"][0][\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ¯ That's a wrap\n",
    "\n",
    "You've now seen how to generate a dataset for instruction tuning. You could use this to:\n",
    "\n",
    "- Generate a dataset for instruction tuning.\n",
    "- Create evaluation datasets for instruction tuning.\n",
    "\n",
    "Next\n",
    "\n",
    "ğŸ§‘â€ğŸ« Learn - About [generating preference datasets](./preference_datasets.md)\n",
    "ğŸ‹ï¸â€â™‚ï¸ Fine-tune a model for instruction tuning with a synthetic dataset based on the [instruction tuning chapter](../../1_instruction_tuning/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smol_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
